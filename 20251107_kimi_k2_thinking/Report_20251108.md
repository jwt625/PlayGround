 Server Information

  - Framework: FastAPI with vLLM backend
  - Version: 0.11.1rc6.dev211+g934a9c3b7
  - Python: 3.12.12
  - Model: kimi-k2-thinking (reasoning model)
  - Max Context: 12,288 tokens

  ---
  âœ… Working Endpoints

  Health & Monitoring

  - GET /health - Health check (200 OK)
  - GET /ping, POST /ping - SageMaker ping check (200 OK)
  - GET /load - Server load metrics (returns {"server_load":0})
  - GET /version - API version info
  - GET /metrics - Prometheus metrics (rich vLLM metrics)

  Model Information

  - GET /v1/models - Lists available models âœ… (requires auth)

  OpenAI-Compatible Chat & Completion

  - POST /v1/chat/completions - Chat completions âœ… (requires auth)
    - Supports system messages
    - Supports multi-turn conversations
    - Supports streaming (with "stream": true)
    - Returns reasoning_content field for thinking model
    - Supports temperature parameter
  - POST /v1/completions - Text completions âœ… (requires auth)
    - Works with prompts
    - Max tokens configuration

  Tokenization

  - POST /tokenize - Convert text to tokens âœ… (no auth required)
    - Example: "Hello, how are you?" â†’ 6 tokens
  - POST /detokenize - Convert tokens to text âœ… (no auth required)

  Anthropic-Compatible API

  - POST /v1/messages - Anthropic Messages API âœ… (requires auth)
    - Returns Anthropic-style responses
    - Supports streaming with Server-Sent Events (SSE)
    - Events: message_start, message_delta, message_stop

  SageMaker Compatibility

  - POST /invocations - SageMaker endpoint âœ… (routes to completions)

  ---
  âŒ Unsupported Endpoints (Model Limitations)

  These endpoints are available but not supported by kimi-k2-thinking (chat/reasoning model):
  - POST /v1/embeddings - "The model does not support Embeddings API"
  - POST /pooling - "The model does not support Pooling API"
  - POST /score, POST /v1/score - "The model does not support Score API"
  - POST /rerank, POST /v1/rerank, POST /v2/rerank - "The model does not support Rerank API"
  - POST /classify - "The model does not support Classification API"

  These would require specialized models (embedding, scoring, or classification models).

  ---
  ðŸ” Authentication

  - Required for: Most generation endpoints (chat, completions, messages, embeddings, etc.)
  - Not required for: Health checks, metrics, tokenization endpoints
  - Method: Bearer token in Authorization header
  - Format: Authorization: Bearer $LLM_TOKEN (stored in .env)

  ---
  ðŸ“Š Key Features Tested

  - âœ… System messages support
  - âœ… Multi-turn conversations
  - âœ… Streaming responses (SSE)
  - âœ… Temperature control
  - âœ… Token limits (max_tokens)
  - âœ… Reasoning content (thinking model feature)
  - âœ… Prometheus metrics monitoring
  - âœ… SageMaker compatibility

  ---
  ðŸ“ˆ Performance Metrics (from /metrics)

  - Requests processed: 1 successful request (before our tests)
  - KV cache usage: 0%
  - Engine state: Awake
  - Time to first token: ~0.98 seconds
  - Inter-token latency: ~0.095 seconds average

  The server is fully operational with comprehensive API support for chat, completion, and tokenization
  tasks!


   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘                    AUTHENTICATION & PERFORMANCE SUMMARY                      â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ENDPOINTS REQUIRING AUTHENTICATION (Bearer Token)                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   âœ“ /v1/models                    - List available models
   âœ“ /v1/chat/completions         - OpenAI-compatible chat completions
   âœ“ /v1/completions              - OpenAI-compatible text completions
   âœ“ /v1/messages                 - Anthropic-compatible messages API
   âœ“ /v1/responses                - Anthropic-compatible responses API
   âœ“ /v1/embeddings               - Embeddings API (model doesn't support)

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ENDPOINTS NOT REQUIRING AUTHENTICATION                                       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   âœ“ /health                       - Health check endpoint
   âœ“ /ping                         - SageMaker ping endpoint
   âœ“ /load                         - Server load metrics
   âœ“ /version                      - API version information
   âœ“ /metrics                      - Prometheus metrics (detailed vLLM stats)
   âœ“ /tokenize                     - Convert text to token IDs
   âœ“ /detokenize                   - Convert token IDs to text
   âœ“ /invocations                  - SageMaker invocations endpoint (!)

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TOKEN THROUGHPUT PERFORMANCE                                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Test 1: Short Response (50 tokens)
     â€¢ Time: 4.95 seconds
     â€¢ Throughput: ~10.1 tokens/second

   Test 2: Medium Response (200 tokens)
     â€¢ Time: 18.87 seconds
     â€¢ Throughput: ~10.6 tokens/second

   Test 3: Long Context (1046 prompt + 300 completion tokens)
     â€¢ Time: 28.44 seconds
     â€¢ Prompt tokens: 1,046
     â€¢ Completion tokens: 300
     â€¢ Throughput: ~10.55 tokens/second

   Test 4: Long Response (500 tokens)
     â€¢ Time: 46.86 seconds
     â€¢ Throughput: ~10.67 tokens/second

   Test 5: Baseline (10 tokens)
     â€¢ Time: 1.18 seconds
     â€¢ Throughput: ~8.5 tokens/second

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PERFORMANCE SUMMARY                                                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Average Throughput: ~10.5 tokens/second (for responses >50 tokens)

   Key Observations:
     â€¢ Very consistent throughput across different response lengths
     â€¢ Slight overhead for very short completions (<20 tokens)
     â€¢ Long context (1000+ tokens) doesn't significantly impact throughput
     â€¢ Time to first token: ~1-2 seconds (from metrics)
     â€¢ Model uses reasoning_content field (thinking model)

   Model Specifications:
     â€¢ Model: kimi-k2-thinking
     â€¢ Max context: 12,288 tokens
     â€¢ Type: Reasoning/thinking model
     â€¢ Backend: vLLM 0.11.1rc6

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RECOMMENDATION                                                               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   For PRODUCTION use:
     â€¢ Use authenticated endpoints (/v1/chat/completions, /v1/messages)
     â€¢ Expect ~10.5 tokens/second generation speed
     â€¢ Use /tokenize to pre-calculate token counts (no auth needed)
     â€¢ Monitor with /metrics endpoint (no auth needed)
     â€¢ Use streaming for better user experience on long responses

   For TESTING/MONITORING:
     â€¢ Use /invocations (no auth) for quick tests
     â€¢ Use /health, /ping for uptime monitoring
     â€¢ Use /load for capacity planning


Requires Bearer Token:
  - All /v1/* generation endpoints (models, chat/completions, completions, messages, responses, embeddings)

  No Authentication Needed:
  - Health/monitoring: /health, /ping, /load, /version, /metrics
  - Tokenization: /tokenize, /detokenize
  - SageMaker endpoint: /invocations (surprisingly works without auth!)

  Performance Metrics

  The server delivers remarkably consistent throughput of approximately 10.5 tokens/second across various
  workloads:

  | Test Scenario           | Tokens | Time   | Throughput  |
  |-------------------------|--------|--------|-------------|
  | Short (50 tokens)       | 50     | 4.95s  | 10.1 tok/s  |
  | Medium (200 tokens)     | 200    | 18.87s | 10.6 tok/s  |
  | Long context (1046+300) | 300    | 28.44s | 10.55 tok/s |
  | Very long (500 tokens)  | 500    | 46.86s | 10.67 tok/s |
  | Baseline (10 tokens)    | 10     | 1.18s  | 8.5 tok/s   |

  Key Findings:
  - Very consistent performance regardless of prompt length (tested up to 1,046 tokens)
  - Time to first token: ~1-2 seconds
  - No significant slowdown with longer context windows
  - Slight overhead for very short completions (<20 tokens)
  - The model is a reasoning/thinking model that outputs in the reasoning_content field

  The server is production-ready with excellent performance characteristics!
