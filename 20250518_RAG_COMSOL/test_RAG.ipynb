{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb237fbe-e60e-4a56-8184-32d04d733568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1758a63-5dcc-49c1-82fd-2ce6f2ea0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1. Extract Text from PDF Files ===\n",
    "def extract_text_from_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def load_all_pdfs_from_directory(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(directory, filename)\n",
    "            print(f\"Extracting: {full_path}\")\n",
    "            text = extract_text_from_pdf(full_path)\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# === 2. Chunk the Text ===\n",
    "def chunk_texts(texts, chunk_size=500, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    documents = splitter.create_documents(texts)\n",
    "    return documents\n",
    "\n",
    "# === 3. Embed the Chunks ===\n",
    "def embed_documents(documents):\n",
    "    embeddings = OpenAIEmbeddings()  # Requires OPENAI_API_KEY env var\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# === 4. Setup RAG ===\n",
    "def setup_rag(vectorstore):\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", k=5)\n",
    "    llm = ChatOpenAI()  # Defaults to gpt-3.5-turbo\n",
    "    qa_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbf7053-aa87-4b75-995a-0fd879795f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: pdf/IntroductionToFuelCellAndElectrolyzerModule.pdf\n",
      "Extracting: pdf/CorrosionModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToPorousMediaFlowModule.pdf\n",
      "Extracting: pdf/IntroductionToPolymerFlowModule.pdf\n",
      "Extracting: pdf/IntroductionToElectrodepositionModule.pdf\n",
      "Extracting: pdf/IntroductionToOptimizationModule.pdf\n",
      "Extracting: pdf/fnp_LicAdmin.pdf\n",
      "Extracting: pdf/IntroductionToThermodynamicProperties.pdf\n",
      "Extracting: pdf/COMSOL_MultiphysicsInstallationGuide.pdf\n",
      "Extracting: pdf/CFDModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToMEMSModule.pdf\n",
      "Extracting: pdf/IntroductionToSubsurfaceFlowModule.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForRevit.pdf\n",
      "Extracting: pdf/IntroductionToLiquidAndGasPropertiesModule.pdf\n",
      "Extracting: pdf/PorousMediaFlowModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToACDCModule.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForPTCCreoParametric.pdf\n",
      "Extracting: pdf/ModelManagerServerManual.pdf\n",
      "Extracting: pdf/ElectrodepositionModuleUsersGuide.pdf\n",
      "Extracting: pdf/ElectrochemistryModuleUsersGuide.pdf\n",
      "Extracting: pdf/BatteryDesignModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToAcousticsModule.pdf\n",
      "Extracting: pdf/ApplicationProgrammingGuide.pdf\n",
      "Extracting: pdf/LiveLinkForRevitUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForInventorUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToMolecularFlowModule.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForAutoCAD.pdf\n",
      "Extracting: pdf/WaveOpticsModuleUsersGuide.pdf\n",
      "Extracting: pdf/MixerModuleUsersGuide.pdf\n",
      "Extracting: pdf/StructuralMechanicsVerificationExamples.pdf\n",
      "Extracting: pdf/IntroductionToPlasmaModule.pdf\n",
      "Extracting: pdf/IntroductionToSemiconductorModule.pdf\n",
      "Extracting: pdf/MEMSModuleUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForSimulinkUsersGuide.pdf\n",
      "Extracting: pdf/PlasmaModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToWaveOpticsModule.pdf\n",
      "Extracting: pdf/IntroductionToChemicalReactionEngineeringModule.pdf\n",
      "Extracting: pdf/COMSOL_SoftwareLicenseAgreement.pdf\n",
      "Extracting: pdf/IntroductionToECADImportModule.pdf\n",
      "Extracting: pdf/IntroductionToCFDModule.pdf\n",
      "Extracting: pdf/MaterialLibraryUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForSolidEdge.pdf\n",
      "Extracting: pdf/ECADImportModuleUsersGuide.pdf\n",
      "Extracting: pdf/NonlinearStructuralMaterialsModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToElectrochemistryModule.pdf\n",
      "Extracting: pdf/UncertaintyQuantificationModuleUsersGuide.pdf\n",
      "Extracting: pdf/StructuralMechanicsModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToCorrosionModule.pdf\n",
      "Extracting: pdf/IntroductionToPipeFlowModule.pdf\n",
      "Extracting: pdf/ACDCModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToParticleTracingModule.pdf\n",
      "Extracting: pdf/RayOpticsModuleUsersGuide.pdf\n",
      "Extracting: pdf/FuelCellAndElectrolyzerModuleUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForPTCCreoParametricUsersGuide.pdf\n",
      "Extracting: pdf/GeomechanicsModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToBatteryDesignModule.pdf\n",
      "Extracting: pdf/AcousticsModuleUsersGuide.pdf\n",
      "Extracting: pdf/COMSOL_ReleaseNotes.pdf\n",
      "Extracting: pdf/IntroductionToDesignModule.pdf\n",
      "Extracting: pdf/IntroductionToCADImportModule.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForSimulink.pdf\n",
      "Extracting: pdf/LiquidAndGasPropertiesModuleUsersGuide.pdf\n",
      "Extracting: pdf/CompositeMaterialsModuleUsersGuide.pdf\n",
      "Extracting: pdf/MetalProcessingModuleUsersGuide.pdf\n",
      "Extracting: pdf/RFModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForExcel.pdf\n",
      "Extracting: pdf/MicrofluidicsModuleUsersGuide.pdf\n",
      "Extracting: pdf/PipeFlowModuleUsersGuide.pdf\n",
      "Extracting: pdf/SemiconductorModuleUsersGuide.pdf\n",
      "Extracting: pdf/ParticleTracingModuleUsersGuide.pdf\n",
      "Extracting: pdf/COMSOL_PhysicsBuilderManual.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForInventor.pdf\n",
      "Extracting: pdf/IntroductionToMicrofluidicsModule.pdf\n",
      "Extracting: pdf/HeatTransferModuleUsersGuide.pdf\n",
      "Extracting: pdf/COMSOL_ProgrammingReferenceManual.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForSOLIDWORKS.pdf\n",
      "Extracting: pdf/COMSOL_ApplicationBuilderManual.pdf\n",
      "Extracting: pdf/IntroductionToApplicationBuilder.pdf\n",
      "Extracting: pdf/FatigueModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToHeatTransferModule.pdf\n",
      "Extracting: pdf/MultibodyDynamicsModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToCOMSOLMultiphysics.pdf\n",
      "Extracting: pdf/ChemicalReactionEngineeringModuleUsersGuide.pdf\n",
      "Extracting: pdf/OptimizationModuleUsersGuide.pdf\n",
      "Extracting: pdf/PolymerFlowModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToRayOpticsModule.pdf\n",
      "Extracting: pdf/MolecularFlowModuleUsersGuide.pdf\n",
      "Extracting: pdf/ModelManagerReferenceManual.pdf\n",
      "Extracting: pdf/IntroductionToLiveLinkForMATLAB.pdf\n",
      "Extracting: pdf/COMSOL_SpecializedTechniquesForPostprocessingAndVisualization.pdf\n",
      "Extracting: pdf/CADImportModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToStructuralMechanicsModule.pdf\n",
      "Extracting: pdf/RotordynamicsModuleUsersGuide.pdf\n",
      "Extracting: pdf/COMSOL_ReferenceManual.pdf\n",
      "Extracting: pdf/DesignModuleUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForAutoCADUsersGuide.pdf\n",
      "Extracting: pdf/gcmma.pdf\n",
      "Extracting: pdf/LiveLinkForExcelUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForSolidEdgeUsersGuide.pdf\n",
      "Extracting: pdf/SubsurfaceFlowModuleUsersGuide.pdf\n",
      "Extracting: pdf/IntroductionToRFModule.pdf\n",
      "Extracting: pdf/LiveLinkForMATLABUsersGuide.pdf\n",
      "Extracting: pdf/LiveLinkForSOLIDWORKSUsersGuide.pdf\n",
      "Extracting: pdf/COMSOL_PostprocessingAndVisualization.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Section 1: Extract and process PDFs\n",
    "pdf_directory = \"pdf\"  # Change this to your PDF directory path\n",
    "texts = load_all_pdfs_from_directory(pdf_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb347261-5bfb-45f3-9bfa-e7651ead012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts saved to extracted_texts.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save texts to a file\n",
    "output_file = \"extracted_texts.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, text in enumerate(texts):\n",
    "        f.write(f\"=== Document {i+1} ===\\n\")\n",
    "        f.write(text)\n",
    "        f.write(\"\\n\\n\")\n",
    "print(f\"Texts saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774705bb-f789-41cb-8c6b-48f129b8ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from sentence-transformers) (0.23.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (0.31.4)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (4.52.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch>=1.11.0->sentence-transformers) (2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6af324e-cde8-4bf1-8c04-49dfb158ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import VLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d2f0c5-aa1c-4999-9ff4-465ae2fb1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load texts\n",
    "with open(\"extracted_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = f.read().split(\"=== Document \")[1:]\n",
    "    texts = [doc.split(\"\\n\", 1)[1] for doc in texts]\n",
    "\n",
    "# Chunk texts\n",
    "def chunk_texts(texts):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    return [Document(page_content=chunk) for text in texts for chunk in splitter.split_text(text)]\n",
    "\n",
    "documents = chunk_texts(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7463b77d-a8f7-4503-ac67-ce0e8d0feba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69717/1632982969.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/ubuntu/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 201.42it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Embeddings with CUDA\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     model_kwargs={\"device\": \"cuda\"}\n",
    "# )\n",
    "\n",
    "# Use Qwen2 embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    model_kwargs={\"device\": \"cuda\"}  # Make sure you have enough GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a891d0-f500-481a-b67c-c65415a46133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 32768, 'do_lower_case': False}) with Transformer model: Qwen2Model \n",
       "  (1): Pooling({'word_embedding_dimension': 3584, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='Alibaba-NLP/gte-Qwen2-7B-instruct', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de47ef5e-643c-41d2-90a0-95beba01d835",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build vectorstore (can be swapped with Chroma if needed)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save to local folder\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vectorstore\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_core/vectorstores/base.py:848\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    846\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:115\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    113\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:720\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 720\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    724\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Build vectorstore (can be swapped with Chroma if needed)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "# Save to local folder\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a259d3-d84c-45a7-9596-223ef7ea1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to Chroma (batched):   6%|▋         | 118/1837 [06:27<1:34:00,  3.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents), batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding to Chroma (batched)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     20\u001b[0m     batch \u001b[38;5;241m=\u001b[39m documents[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save to disk\u001b[39;00m\n\u001b[1;32m     24\u001b[0m vectorstore\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_core/vectorstores/base.py:288\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    287\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m )\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:115\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    113\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/20250520_RAG_COMSOL/rag-env/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:720\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 720\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    724\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# chroma = Chroma(embedding_function=embeddings, persist_directory=\"chroma_db\")\n",
    "\n",
    "# for doc in tqdm(documents, desc=\"Adding to Chroma\"):\n",
    "#     chroma.add_documents([doc])\n",
    "# chroma.persist()\n",
    "\n",
    "# batched\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup Chroma\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"chroma_db\")\n",
    "\n",
    "# Batched insertion\n",
    "batch_size = 64\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"Adding to Chroma (batched)\"):\n",
    "    batch = documents[i:i + batch_size]\n",
    "    vectorstore.add_documents(batch)\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c3bf3-e9c2-4f5b-9af4-5a4c4d086227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use DeepSeek LLM via vLLM\n",
    "llm = VLLM(\n",
    "    model=\"deepseek-ai/deepseek-llm-7b-chat\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Build RAG chain\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# Run query\n",
    "response = qa.run(\"Summarize this corpus.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-env)",
   "language": "python",
   "name": "rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
