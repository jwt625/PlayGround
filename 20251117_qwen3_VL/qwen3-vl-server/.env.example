# vLLM Server Configuration Example
# Copy this file to .env and customize the values

# Model configuration
MODEL_NAME=Qwen/Qwen3-VL-32B-Instruct

# Server configuration
HOST=0.0.0.0
PORT=8000

# Authentication - IMPORTANT: Generate a secure random token!
# This token is required for all API requests
API_KEY=your-secret-api-key-here

# GPU configuration for H100
# TENSOR_PARALLEL_SIZE: Number of GPUs to use (1 for single GPU, 2 for both H100s)
TENSOR_PARALLEL_SIZE=1

# GPU_MEMORY_UTILIZATION: Fraction of GPU memory to use (0.0-1.0)
GPU_MEMORY_UTILIZATION=0.95

# MAX_MODEL_LEN: Maximum context length (default 262K, but 128K is good for most cases)
MAX_MODEL_LEN=128000

# DTYPE: Data type for model weights (bfloat16, float16, auto)
DTYPE=bfloat16

# Performance optimizations
ASYNC_SCHEDULING=true

# Disable video processing if only using images (set to true to disable video)
DISABLE_VIDEO=false

